{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanbizet/Documents/question_type_examen/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://jv6gjfb6zh.execute-api.eu-west-3.amazonaws.com/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_by_theme(theme):\n",
    "    try:\n",
    "        response = requests.get(f\"{api_url}/theme/{theme}\")\n",
    "        response.raise_for_status()  # Raise an exception for non-2xx status codes\n",
    "        data = list()\n",
    "        if theme == \"TYPE-EXAMEN\":\n",
    "            for q in response.json():\n",
    "                if \"&&\" not in  q[\"answer\"] :\n",
    "                    data.append(q)\n",
    "            return data\n",
    "        return response.json()\n",
    "    except RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_examen = get_question_by_theme(\"TYPE-EXAMEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event(theme):\n",
    "    try:\n",
    "        response = requests.get(f\"{api_url}/event\", params={'theme': theme})\n",
    "        response.raise_for_status()  # Raise an exception for non-2xx status codes\n",
    "\n",
    "        return response.json()\n",
    "    except RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from io import StringIO\n",
    "sio = StringIO(json.dumps(get_event(\"TYPE-EXAMEN\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_event = pd.DataFrame(get_event(\"TYPE-EXAMEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['theme', 'id-event', 'event-type', 'id-question', 'timestamp'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_event.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_event = pd.DataFrame(get_event(\"TYPE-EXAMEN\"))\n",
    "df_event_ko = df_event[df_event['event-type'] == \"KO\" ]\n",
    "most_ko_q=df_event_ko.groupby(by='id-question')\\\n",
    "          .count()[[\"id-event\"]]\\\n",
    "          .sort_values(by=\"id-event\", ascending=False)\\\n",
    "          .head(10).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = list(set(map(lambda x : x[\"id-question\"],question_examen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_nerver_asked = list(filter(lambda x : x not in set(df_event[\"id-question\"]), question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event['timestamp'] = pd.to_datetime(df_event['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103    a2d57b16-defc-4493-937f-8caed883dbba\n",
       "129    679f31d8-821e-4db7-adf1-d6d4773a98e4\n",
       "139    d1c32481-94ad-46c8-bf29-b2fabccd5065\n",
       "55     9d5b595c-6818-4f77-9d93-a7776e4d3fb9\n",
       "82     918daf85-eea9-4229-aad3-3645446d0a36\n",
       "                       ...                 \n",
       "114    d521e940-7c11-41b8-9b2c-b230823939b1\n",
       "68     05dead38-24b7-4ee2-b551-28d37f03b5d8\n",
       "107    69199ad3-684a-4e9d-b4f2-9b7476866479\n",
       "25     8894a76a-e69c-4345-8cb5-8958b246fe9a\n",
       "122    8894a76a-e69c-4345-8cb5-8958b246fe9a\n",
       "Name: id-question, Length: 152, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_event.sort_values(by=[\"timestamp\"])['id-question'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_question = \"a2d57b16-defc-4493-937f-8caed883dbba\"\n",
    "response = requests.get(f\"{api_url}/question/{id_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theme': 'TYPE-EXAMEN',\n",
       " 'question': 'A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance. Which solution meets these requirements MOST cost-effectively?',\n",
       " 'options': ['Stop the DB instance when tests are completed. Restart the DB instance when required.',\n",
       "  'Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.',\n",
       "  'Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.',\n",
       "  'Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required.'],\n",
       " 'img_path': None,\n",
       " 'answer': 'Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.',\n",
       " 'id-question': 'a2d57b16-defc-4493-937f-8caed883dbba'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "selected_id_question = list()\n",
    "df_event = pd.DataFrame(get_event(\"TYPE-EXAMEN\"))\n",
    "df_event_ko = df_event[df_event['event-type'] == \"KO\"]\n",
    "most_ko_question = df_event_ko.groupby(by='id-question')\\\n",
    "    .count()[[\"id-event\"]]\\\n",
    "    .sort_values(by=\"id-event\", ascending=False)\\\n",
    "    .head(10).index.to_list()\n",
    "response = requests.get(f\"{api_url}/theme/TYPE-EXAMEN\")\n",
    "question_examen = response.json()\n",
    "question = list(set(map(lambda x: x[\"id-question\"], question_examen)))\n",
    "question_nerver_asked = list(\n",
    "    filter(lambda x: x not in set(df_event[\"id-question\"]), question))\n",
    "asked_question = df_event.sort_values(by=[\"timestamp\"])[\n",
    "    'id-question'].to_list()\n",
    "selected_id_question += question_nerver_asked\n",
    "selected_id_question += most_ko_question\n",
    "if len(selected_id_question) > 65:\n",
    "    selected_id_question = selected_id_question[:65]\n",
    "elif len(selected_id_question) < 65:\n",
    "    selected_id_question += asked_question[:65-len(selected_id_question)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company runs its infrastructure on AWS and has a registered base of 700,000 users for its document management application. The company intends to create a product that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A solutions architect must design a scalable solution to accommodate demand that will grow rapidly over time. Which solution meets these requirements MOST cost-effectively?',\n",
       "  'options': ['Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.',\n",
       "   'Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store them back in DynamoDB.',\n",
       "   'Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the EBS store.',\n",
       "   'Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS store.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.',\n",
       "  'id-question': '41e4cfec-21d9-4630-8e20-4d89e0cd16f1'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company has a data ingestion workflow that consists of the following: • An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries • An AWS Lambda function to process the data and record metadata. The company observes that the ingestion workflow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not ingest the corresponding data unless the company manually reruns the job. Which combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future? (Choose two.)',\n",
       "  'options': ['Increase the CPU and memory that are allocated to the Lambda function.',\n",
       "   'Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.',\n",
       "   'Increase provisioned throughput for the Lambda function.',\n",
       "   'Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue',\n",
       "   'Deploy the Lambda function in multiple Availability Zones.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.&&Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue',\n",
       "  'id-question': 'd29a6f76-a912-46ec-aaa9-165108d469e2'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company’s website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure that the catalog is highly available and that the catalog is stored in a durable location. What should a solutions architect do to meet these requirements?',\n",
       "  'options': ['Move the catalog to Amazon ElastiCache for Redis.',\n",
       "   'Deploy a larger EC2 instance with a larger instance store.',\n",
       "   'Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.',\n",
       "   'Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.',\n",
       "  'id-question': 'c46accac-3488-480e-bdde-872bfcad4f4b'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)',\n",
       "  'options': ['Amazon Simple Queue Service (Amazon SQS)',\n",
       "   'Amazon Kinesis',\n",
       "   'Amazon Simple Notification Service (Amazon SNS)',\n",
       "   'Amazon DynamoDB',\n",
       "   'Amazon S3'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Amazon DynamoDB&&Amazon S3',\n",
       "  'id-question': '39d1cf7f-d808-44a3-9f3d-c4ded5eb15c9'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'An engineering team wants to examine the feasibility of the user data feature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)',\n",
       "  'options': ['By default, user data runs only during the boot cycle when you first launch an instance',\n",
       "   'By default, scripts entered as user data do not have root user privileges for executing',\n",
       "   'When an instance is running, you can update user data by using root user credentials',\n",
       "   'By default, scripts entered as user data are executed with root user privileges',\n",
       "   'By default, user data is executed every time an Amazon EC2 instance is re-started'],\n",
       "  'img_path': None,\n",
       "  'answer': 'By default, user data runs only during the boot cycle when you first launch an instance&&By default, scripts entered as user data are executed with root user privileges',\n",
       "  'id-question': 'bb02f515-50c6-40d8-89c9-4ee3e60eb29a'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company’s dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site’s backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed. What should the solutions architect recommend?',\n",
       "  'options': ['Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.',\n",
       "   'Move the website to Amazon S3. Use Cross-Region Replication between Regions.',\n",
       "   'Use Amazon CloudFront with a custom origin pointing to the on-premises servers.',\n",
       "   'Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Amazon CloudFront with a custom origin pointing to the on-premises servers.',\n",
       "  'id-question': '61506d63-8359-40ba-9d31-c1605e47f808'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs. How can the solutions architect meet this requirement?',\n",
       "  'options': ['Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it.',\n",
       "   'Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets',\n",
       "   'Deploy the application into a public subnet and allow it to route through an internet gateway to access the S3 buckets.',\n",
       "   'Deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to the S3 buckets.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets',\n",
       "  'id-question': 'c7c4a7b8-b82d-4fe9-bb1f-599f4da10f16'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A solutions architect is designing a new hybrid architecture to extend a company’s on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails. What should the solutions architect do to meet these requirements?',\n",
       "  'options': ['Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.',\n",
       "   'Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.',\n",
       "   'Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails.',\n",
       "   'Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.',\n",
       "  'id-question': 'e3883977-1fe3-4a15-ac59-6901edea89ab'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability. How should a solutions architect design the architecture to meet these requirements?',\n",
       "  'options': ['Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.',\n",
       "   'Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.',\n",
       "   'Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes.',\n",
       "   'Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.',\n",
       "  'id-question': '345d3ea9-0274-45b7-852e-a76cfb7a100d'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'How can you change the instance type used in Auto Scaling Group?',\n",
       "  'options': ['Instances should be stopped and then type can be changed',\n",
       "   'AS Group should be deleted and recreated',\n",
       "   'It is not possible to change the instance type',\n",
       "   'A new launch configuration with a new instance type should be created and attached to AS group'],\n",
       "  'img_path': None,\n",
       "  'answer': 'AS Group should be deleted and recreated&&A new launch configuration with a new instance type should be created and attached to AS group',\n",
       "  'id-question': '489ff74e-e243-4031-8f65-fd250d3af22c'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': \"An international travel-booking service company that sees 100 million unique users monthly for their web-app, has built and deployed its application in amazonEC2 behind Elastic Load Balancer (ELB). To manage the surge in traffic, EC2 instances are configured with Auto Scaling Groups. To improve the user experience and resolve latency, downtime related issues for global customers, the company is looking for a cross-region traffic management solution to route user traffic to the optimal endpoint based on performance, user's location, and instant reaction to the changes in application health. You have been hired as a Solution Architect to implement this solution. Which is the best option in your opinion?\",\n",
       "  'options': ['Modify the application from its existing platform to AWS Serverless (API Gateway, Lambda, DynamoDB, etc.) to handle the dynamic load, solve latency issues and improve user experience.',\n",
       "   'Place Amazon CloudFront in front of the ELB to enable edge location cache for low latency and better user experience.',\n",
       "   'Use AWS Global Accelerator in front of ELB to improve the availability, performance, and user experience.',\n",
       "   'Use AWS NetworkLoadBalancer to handle a high volume of traffic and achieve low latency. This will also improve user experience.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use AWS Global Accelerator in front of ELB to improve the availability, performance, and user experience.',\n",
       "  'id-question': 'b84fc10d-d877-4ef9-a49e-3118dfeed365'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company is storing sensitive user information in an Amazon S3 bucket. The company wants to provide secure access to this bucket from the application tier running on Amazon EC2 instances inside a VPC. Which combination of steps should a solutions architect take to accomplish this? (Choose two.)',\n",
       "  'options': ['Configure a VPC gateway endpoint for Amazon S3 within the VPC.',\n",
       "   'Create a bucket policy to make the objects in the S3 bucket public.',\n",
       "   'Create a bucket policy that limits access to only the application tier running in the VPC',\n",
       "   'Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance.',\n",
       "   'Create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Configure a VPC gateway endpoint for Amazon S3 within the VPC.&&Create a bucket policy that limits access to only the application tier running in the VPC',\n",
       "  'id-question': 'cdda94cf-9912-4585-860a-75bc06f1f16c'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)',\n",
       "  'options': ['Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.',\n",
       "   'Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application’s API for the data.',\n",
       "   'Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.',\n",
       "   'Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application’s API for the data.',\n",
       "   'Configure the application to send the data to Amazon Kinesis Data Firehose.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.&&Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application’s API for the data.',\n",
       "  'id-question': 'f42f2c87-b0d3-4bc8-a58b-16a72299347b'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time. What should a solutions architect propose to ensure users see all of their documents at once?',\n",
       "  'options': ['Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server',\n",
       "   'Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS',\n",
       "   'Copy the data so both EBS volumes contain all the documents',\n",
       "   'Configure the Application Load Balancer to direct a user to the server with the documents'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS',\n",
       "  'id-question': '53460410-2f33-406a-afce-b099ef125d61'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?',\n",
       "  'options': ['Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN',\n",
       "   'Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN',\n",
       "   'Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN',\n",
       "   'Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN',\n",
       "  'id-question': '03fd44b0-f0ad-426c-8230-2d88e24873a2'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis. The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days. What is the MOST operationally efficient solution that meets these requirements?',\n",
       "  'options': ['Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.',\n",
       "   'Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.',\n",
       "   'Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.',\n",
       "   'Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.',\n",
       "  'id-question': '97faa48e-ee24-47ce-91e5-cb5386f05d32'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company has an on-premises application that generates a large amount of time-sensitive data that is backed up to Amazon S3. The application has grown and there are user complaints about internet bandwidth limitations. A solutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for internal users. Which solution meets these requirements?',\n",
       "  'options': ['Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint.',\n",
       "   'Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.',\n",
       "   'Order daily AWS Snowball devices. Load the data onto the Snowball devices and return the devices to AWS each day.',\n",
       "   'Submit a support ticket through the AWS Management Console. Request the removal of S3 service limits from the account.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.',\n",
       "  'id-question': '8da6c009-b906-4d20-8b6c-cfad8cf53616'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'Which record type queries are free when using Route 53?',\n",
       "  'options': ['MX', 'TXT', 'Alias', 'AAAA'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Alias',\n",
       "  'id-question': 'd5759720-cd3e-4aa2-ba5d-b502a75f023e'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company needs to perform asynchronous processing, and has Amazon SQS as part of a decoupled architecture. The company wants to ensure that the number of empty responses from polling requests is kept to a minimum. What should a solutions architect do to ensure that empty responses are reduced?',\n",
       "  'options': ['Increase the receive message wait time for the queue',\n",
       "   'Increase the maximum message retention period for the queue',\n",
       "   'Increase the default visibility timeout for the queue',\n",
       "   'Increase the maximum receives for the redrive policy for the queue'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Increase the receive message wait time for the queue',\n",
       "  'id-question': '6e1e1008-8558-4a42-b626-499924b9d566'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?',\n",
       "  'options': ['Use Amazon Aurora Global Database to enable fast local reads with low latency in each region',\n",
       "   'Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases',\n",
       "   'Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters',\n",
       "   'Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Amazon Aurora Global Database to enable fast local reads with low latency in each region',\n",
       "  'id-question': 'aa3d5198-88c4-4c2e-beae-f9facbd4bc38'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?',\n",
       "  'options': ['Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift',\n",
       "   'Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift',\n",
       "   'Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift',\n",
       "   'Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift',\n",
       "  'id-question': '7efb2381-2487-43a2-ba07-7e52b69e4224'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two)',\n",
       "  'options': ['Enable versioning on the Amazon S3 bucket',\n",
       "   'Establish a process to get managerial approval for deleting Amazon S3 objects',\n",
       "   'Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object',\n",
       "   'Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket',\n",
       "   'Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Enable versioning on the Amazon S3 bucket&&Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket',\n",
       "  'id-question': 'abea1090-f891-4b55-97cc-4926c93f70e7'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)',\n",
       "  'options': ['NAT instance supports port forwarding',\n",
       "   'NAT instance can be used as a bastion server',\n",
       "   'NAT gateway supports port forwarding',\n",
       "   'Security Groups can be associated with a NAT instance',\n",
       "   'NAT gateway can be used as a bastion server',\n",
       "   'Security Groups can be associated with a NAT gateway'],\n",
       "  'img_path': None,\n",
       "  'answer': 'NAT instance supports port forwarding&&NAT instance can be used as a bastion server&&Security Groups can be associated with a NAT instance',\n",
       "  'id-question': '6845dbb5-32e8-4c5f-90aa-fdb9e16645c2'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company wants to migrate its on-premises application to AWS. The application produces output files that vary in size from tens of gigabytes to hundreds of terabytes. The application data must be stored in a standard file system structure. The company wants a solution that scales automatically, is highly available, and requires minimum operational overhead. Which solution will meet these requirements?',\n",
       "  'options': ['Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.',\n",
       "   'Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage.',\n",
       "   'Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.',\n",
       "   'Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.',\n",
       "  'id-question': '51d7c060-063a-4867-8dcc-07a95bf664d4'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week. What should the company do to guarantee the EC2 capacity?',\n",
       "  'options': ['Purchase Reserved Instances that specify the Region needed.',\n",
       "   'Create an On-Demand Capacity Reservation that specifies the Region needed.',\n",
       "   'Purchase Reserved Instances that specify the Region and three Availability Zones needed.',\n",
       "   'Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.',\n",
       "  'id-question': 'eb2a6ea1-1d6b-4e19-879e-d327f1d52dbb'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes. What should a solutions architect do to accomplish this goal?',\n",
       "  'options': ['Turn on AWS Config with the appropriate rules.',\n",
       "   'Turn on AWS Trusted Advisor with the appropriate checks.',\n",
       "   'Turn on Amazon Inspector with the appropriate assessment template.',\n",
       "   'Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events).'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Turn on AWS Config with the appropriate rules.',\n",
       "  'id-question': 'ee8a6a22-42b1-4e0b-b881-bfcceded9eae'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)',\n",
       "  'options': ['Use AWS Direct Connect',\n",
       "   'Use Amazon Aurora Replica',\n",
       "   'Use AWS Shield',\n",
       "   'Use AWS Global Accelerator',\n",
       "   'Use Amazon CloudFront distribution in front of the Application Load Balancer'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Amazon Aurora Replica&&Use Amazon CloudFront distribution in front of the Application Load Balancer',\n",
       "  'id-question': '7163978e-a201-43ec-b431-2941c8dd065c'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company’s domain name and corresponding certificate so that the third-party services can use HTTPS. Which solution will meet these requirements?',\n",
       "  'options': [\"Create stage variables in API Gateway with Name='Endpoint-URL' and Value='Company Domain Name' to overwrite the default URL. Import the public certificate associated with the company’s domain name into AWS Certificate Manager (ACM).\",\n",
       "   'Create Route 53 DNS records with the company’s domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company’s domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.',\n",
       "   'Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company’s domain name. Import the public certificate associated with the company’s domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.',\n",
       "   'Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company’s domain name. Import the public certificate associated with the company’s domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company’s domain name. Point an A record to the company’s domain name.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company’s domain name. Import the public certificate associated with the company’s domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.',\n",
       "  'id-question': 'ed1d8fbf-7e99-4d0d-9bd3-15e02e7267da'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?',\n",
       "  'options': ['Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams',\n",
       "   'Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams',\n",
       "   'Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams',\n",
       "   'Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams',\n",
       "  'id-question': '6a768898-fe1f-49f0-8f83-ca2710e97a98'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?',\n",
       "  'options': [\"Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI\",\n",
       "   'Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB',\n",
       "   'Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB',\n",
       "   'Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB',\n",
       "  'id-question': 'ee7a9df8-7454-49df-a06e-caae4e7d7e81'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity.  As a solutions architect, which of the following solutions would you recommend to help address the given requirements?',\n",
       "  'options': ['Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3.',\n",
       "   'Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3.',\n",
       "   'Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3.',\n",
       "   'Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3.',\n",
       "  'id-question': 'd5f73a0b-4bd8-4e59-9b5b-ee752572f539'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received. Which solution will meet these requirements?',\n",
       "  'options': ['Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the topic to perform processing.',\n",
       "   'Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.',\n",
       "   'Use an API Gateway authorizer to block any requests while the application processes an order.',\n",
       "   'Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order. Configure the SQS standard queue to invoke an AWS Lambda function for processing.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.',\n",
       "  'id-question': '41ecc636-4963-4480-b179-f28c62e2be53'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC. A solutions architect needs to connect from the on-premises network, through the company’s internet connection, to the bastion host, and to the application servers. The solutions architect must make sure that the security groups of all the EC2 instances will allow that access. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)',\n",
       "  'options': ['Replace the current security group of the bastion host with one that only allows inbound access from the application instances.',\n",
       "   'Replace the current security group of the bastion host with one that only allows inbound access from the internal IP range for the company.',\n",
       "   'Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.',\n",
       "   'Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.',\n",
       "   'Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.&&Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.',\n",
       "  'id-question': '6ae533e4-ad00-4ff2-9309-b773f5e0121d'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system.  Which of the following represents the MOST operationally efficient way to meet this requirement?',\n",
       "  'options': ['Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours',\n",
       "   'Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours',\n",
       "   'Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system',\n",
       "   'Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours',\n",
       "  'id-question': 'e4bb1a5f-c53f-4c6d-9212-0e4ad45c6ef7'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application’s elasticity and availability. The current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company’s development team pulls a full export of the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development team is unable to use the staging environment until the procedure completes. A solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the development team the ability to continue using the staging environment without delay. Which solution meets these requirements?',\n",
       "  'options': ['Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.',\n",
       "   'Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.',\n",
       "   'Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.',\n",
       "   'Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.',\n",
       "  'id-question': '471dc38c-fd25-494f-88cc-4db352b6af73'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'You have an application running in us-west-2 that requires six Amazon Elastic Compute Cloud (EC2) instances running at all times. With three AZs available in that region (us-west-2a, us-west-2b, and us-west-2c), which of the following deployments provides 100 percent fault tolerance if any single AZ in us-west-2 becomes unavailable? Choose 2 answers',\n",
       "  'options': ['Us-west-2a with three EC2 instances, us-west-2b with three EC2 instances, and us-west-2c with three EC2 instances.',\n",
       "   'Us-west-2a with four EC2 instances, us-west-2b with two EC2 instances, and us-west-2c with two EC2-instances',\n",
       "   'Us-west-2a with two EC2 instances, us-west-2b with two EC2 instances, and us-west-2c with two EC2 instances',\n",
       "   '6 EC2 Instances in us-west-2a, 6 EC2 Instances in us-west-2b, and no EC2 Instances in us-west-2c'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Us-west-2a with two EC2 instances, us-west-2b with two EC2 instances, and us-west-2c with two EC2 instances&&6 EC2 Instances in us-west-2a, 6 EC2 Instances in us-west-2b, and no EC2 Instances in us-west-2c',\n",
       "  'id-question': 'a045a993-61ef-448a-907a-f6bffb4329bb'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)',\n",
       "  'options': ['Store the installation files in Amazon S3 so they can be quickly retrieved',\n",
       "   'Use Amazon EC2 user data to customize the dynamic installation parts at boot time',\n",
       "   'Use Amazon EC2 user data to install the application at boot time',\n",
       "   'Use AWS Elastic Beanstalk deployment caching feature',\n",
       "   'Create a Golden Amazon Machine Image (AMI) with the static installation components already setup'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Amazon EC2 user data to customize the dynamic installation parts at boot time&&Create a Golden Amazon Machine Image (AMI) with the static installation components already setup',\n",
       "  'id-question': '59c66c54-c556-4d01-bad5-6b20dcdec2e0'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS Organizations. The company’s security team needs a single sign-on (SSO) solution across all the company’s accounts. The company must continue managing the users and groups in its on-premises self-managed Microsoft Active Directory. Which solution will meet these requirements?',\n",
       "  'options': ['Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company’s self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.',\n",
       "   'Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company’s self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.',\n",
       "   'Use AWS Directory Service. Create a two-way trust relationship with the company’s self-managed Microsoft Active Directory.',\n",
       "   'Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company’s self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.',\n",
       "  'id-question': '2424c7d5-58d3-4d65-bccc-f8151a3ccd15'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)',\n",
       "  'options': ['Enable versioning on the S3 bucket.',\n",
       "   'Enable MFA Delete on the S3 bucket.',\n",
       "   'Create a bucket policy on the S3 bucket.',\n",
       "   'Enable default encryption on the S3 bucket.',\n",
       "   'Create a lifecycle policy for the objects in the S3 bucket.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Enable versioning on the S3 bucket.&&Enable MFA Delete on the S3 bucket.',\n",
       "  'id-question': '1b30f638-cc54-40ee-8eb5-9b9ca4b54e56'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'Is it possible to create an AMI while an instance is running?',\n",
       "  'options': ['Yes, AMI can be created without any change',\n",
       "   'No, instance should be stopped and rebooted',\n",
       "   'Yes, only if it is Linux instance',\n",
       "   \"Yes, if only 'no reboot' option is checked\"],\n",
       "  'img_path': None,\n",
       "  'answer': \"Yes, if only 'no reboot' option is checked\",\n",
       "  'id-question': '80b7a40b-56ac-42d2-9431-da61e76ef51a'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company. How should security groups be configured in this situation? (Choose two.)',\n",
       "  'options': ['Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.',\n",
       "   'Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.',\n",
       "   'Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.',\n",
       "   'Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier.',\n",
       "   'Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.&&Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.',\n",
       "  'id-question': 'ab0f0193-dcd7-4e4c-955e-7a1005cac9e8'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company is implementing a shared storage solution for a gaming application that is hosted in an on-premises data center. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?',\n",
       "  'options': ['Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.',\n",
       "   'Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.',\n",
       "   'Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.',\n",
       "   'Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.',\n",
       "  'id-question': 'fb8fff8f-f132-4fa1-b671-4db2f0ee1890'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources. What should a solutions architect do to meet these requirements?',\n",
       "  'options': ['Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.',\n",
       "   'Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.',\n",
       "   'Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.',\n",
       "   'Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.',\n",
       "  'id-question': 'ac4761d8-1ad5-4694-a67d-e9186295641a'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)',\n",
       "  'options': ['AWS Step Functions',\n",
       "   'AWS Lambda',\n",
       "   'Amazon CloudWatch',\n",
       "   'Amazon Simple Queue Service (Amazon SQS)',\n",
       "   'Amazon Simple Notification Service (Amazon SNS)'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Amazon CloudWatch&&Amazon Simple Notification Service (Amazon SNS)',\n",
       "  'id-question': 'ab1d0d54-e478-47ce-bce8-342817cd00e2'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)',\n",
       "  'options': ['AWS Glue',\n",
       "   'AWS Database Migration Service (AWS DMS)',\n",
       "   'Basic Schema Copy',\n",
       "   'AWS Schema Conversion Tool (AWS SCT)',\n",
       "   'AWS Snowball Edge'],\n",
       "  'img_path': None,\n",
       "  'answer': 'AWS Database Migration Service (AWS DMS)&&AWS Schema Conversion Tool (AWS SCT)',\n",
       "  'id-question': '6a21fe1a-8818-4d71-9345-643325980665'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours. The company’s development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must recommend a solution to resolve this issue. Which solution will meet this requirement with the LEAST operational overhead?',\n",
       "  'options': ['Modify the DB instance to be a Multi-AZ deployment.',\n",
       "   'Create a read replica of the database. Configure the script to query only the read replica.',\n",
       "   'Instruct the development team to manually export the entries in the database at the end of each day.',\n",
       "   'Use Amazon ElastiCache to cache the common queries that the script runs against the database.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Create a read replica of the database. Configure the script to query only the read replica.',\n",
       "  'id-question': 'ed3c5be9-5017-4c80-8da7-9767763848df'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour with millisecond latency during peak hours. Which solution will meet these requirements with the LEAST operational overhead?',\n",
       "  'options': ['Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.',\n",
       "   'Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) to distribute the website traffic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.',\n",
       "   'Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.',\n",
       "   'Use an Amazon S3 bucket to host the website’s static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use an Amazon S3 bucket to host the website’s static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB.',\n",
       "  'id-question': '56f87daf-9c7c-43c6-83f9-bfa238728581'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'An application allows users at a company’s headquarters to access product data. The product data is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an application performance slowdown and wants to separate read traffic from write traffic. A solutions architect needs to optimize the application’s performance quickly. What should the solutions architect recommend?',\n",
       "  'options': ['Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary Availability Zone.',\n",
       "   'Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary Availability Zone.',\n",
       "   'Create read replicas for the database. Configure the read replicas with half of the compute and storage resources as the source database.',\n",
       "   'Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database.',\n",
       "  'id-question': '15cea1a8-5610-40f1-9eb5-c0152d41ab28'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective for hosting the website?',\n",
       "  'options': ['Containerize the website and host it in AWS Fargate.',\n",
       "   'Create an Amazon S3 bucket and host the website there.',\n",
       "   'Deploy a web server on an Amazon EC2 instance to host the website.',\n",
       "   'Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Create an Amazon S3 bucket and host the website there.',\n",
       "  'id-question': '7e122576-b76a-469e-90ee-646f02130461'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': \"A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?\",\n",
       "  'options': ['Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances',\n",
       "   'Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies',\n",
       "   'Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs',\n",
       "   'Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations',\n",
       "  'id-question': '2834ebac-5b08-46e1-b78b-e5be876377cb'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?',\n",
       "  'options': ['Use Elastic Load Balancing (ELB) to distribute traffic across deployments',\n",
       "   'Use Amazon Route 53 weighted routing to spread traffic across different deployments',\n",
       "   'Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment',\n",
       "   'Use AWS CodeDeploy deployment options to choose the right deployment'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment',\n",
       "  'id-question': '9b1c2005-6514-44cb-ad07-2a66340c00c7'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture. What should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?',\n",
       "  'options': ['Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.',\n",
       "   'Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.',\n",
       "   'Use Amazon Athena directly with Amazon S3 to run the queries as needed.',\n",
       "   'Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Amazon Athena directly with Amazon S3 to run the queries as needed.',\n",
       "  'id-question': 'a423a009-4168-4d01-9468-7723ff7e11db'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?',\n",
       "  'options': ['Use AWS EMR to replicate the data from the databases into Amazon Redshift',\n",
       "   'Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift',\n",
       "   'Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift',\n",
       "   'Use AWS Glue to replicate the data from the databases into Amazon Redshift'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift',\n",
       "  'id-question': '88bcf80d-06f5-4422-90a7-edfa3135a119'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company has several web servers that need to frequently access a common Amazon RDS MySQL Multi-AZ DB instance. The company wants a secure method for the web servers to connect to the database while meeting a security requirement to rotate user credentials frequently.  Which solution meets these requirements?',\n",
       "  'options': ['Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the web servers to access AWS Secrets Manager.',\n",
       "   'Store the database user credentials in AWS Systems Manager OpsCenter. Grant the necessary IAM permissions to allow the web servers to access OpsCenter.',\n",
       "   'Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to allow the web servers to retrieve credentials and access the database.',\n",
       "   'Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the web server file system. The web server should be able to decrypt the files and access the database.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the web servers to access AWS Secrets Manager.',\n",
       "  'id-question': '9397b186-f8cc-4bb4-9b51-5907b63137c5'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The company is concerned about data transfer charges. What is the MOST cost-effective way for the company to avoid Regional data transfer charges?',\n",
       "  'options': ['Launch the NAT gateway in each Availability Zone.',\n",
       "   'Replace the NAT gateway with a NAT instance.',\n",
       "   'Deploy a gateway VPC endpoint for Amazon S3.',\n",
       "   'Provision an EC2 Dedicated Host to run the EC2 instances.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Deploy a gateway VPC endpoint for Amazon S3.',\n",
       "  'id-question': '1cb8fb17-1b81-49bf-b3a5-9bc913e24458'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?',\n",
       "  'options': ['Amazon Redshift',\n",
       "   'Amazon Neptune',\n",
       "   'Amazon Aurora',\n",
       "   'Amazon OpenSearch Service'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Amazon Neptune',\n",
       "  'id-question': 'f32d0653-bb6f-447c-b7b4-d90999817097'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)',\n",
       "  'options': ['Amazon RDS',\n",
       "   'Amazon DynamoDB',\n",
       "   'Amazon Redshift',\n",
       "   'Amazon ElastiCache',\n",
       "   'AWS Lambda'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Amazon DynamoDB&&AWS Lambda',\n",
       "  'id-question': '67f3bd71-ff60-4dd6-83c9-e3459741f27b'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A meteorological company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents. The average volume of data that the company collects from each site daily is 500 GB. Each site has a high-speed Internet connection. The company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon S3 bucket. The solution must minimize operational complexity. Which solution meets these requirements?',\n",
       "  'options': ['Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.',\n",
       "   'Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.',\n",
       "   'Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket.',\n",
       "   'Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. At regular intervals, take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that Region.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.',\n",
       "  'id-question': '416ac3a1-8b63-4e3b-b731-93a9d1db2652'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A customer’s nightly EMR job processes a single 2TB data file stored on Amazon Simple Storage Service (S3). The Amazon Elastic Map Reduce (EMR) job runs on two On-Demand core nodes and three On-Demand task nodes. Which of the following may help reduce the EMR job completion time? Choose 2 answers',\n",
       "  'options': ['Use a bootstrap action the present the S3 bucket as a local filesystem.',\n",
       "   'Change the input split size in the MapReduce job configuration',\n",
       "   'Enable termination protection for the job flow.',\n",
       "   'Launch the core nodes and task nodes within an Amazon Virtual Cloud',\n",
       "   'Use three Spot Instances rather than three On-Demand instances for the taks nodes.',\n",
       "   'Adjust the number of simultaneous mapper tasks'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Change the input split size in the MapReduce job configuration&&Adjust the number of simultaneous mapper tasks',\n",
       "  'id-question': '99055ed6-0bea-437b-9d23-9b5c425ce2e6'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?',\n",
       "  'options': ['Use Internet Gateway to access Amazon SQS',\n",
       "   'Use Network Address Translation (NAT) instance to access Amazon SQS',\n",
       "   'Use VPN connection to access Amazon SQS',\n",
       "   'Use VPC endpoint to access Amazon SQS'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use VPC endpoint to access Amazon SQS',\n",
       "  'id-question': '533384de-00e3-4094-beb3-82788dfc99ba'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company’s security team requires that all data stored in the cloud be encrypted at rest at all times using encryption keys stored on-premises. Which encryption options meet these requirements? (Select TWO.)',\n",
       "  'options': ['Use Server-Side Encryption with AWS KMS Managed Keys (SSE-KMS).',\n",
       "   'Use an AWS Lambda function triggered by Amazon S3 events to encrypt the data using the customer’s keys.',\n",
       "   'Use Server-Side Encryption with Customer Provided Keys (SSE-C).',\n",
       "   'Use client-side encryption to provide at-rest encryption.',\n",
       "   'Use Server-Side Encryption with Amazon S3 Managed Keys (SSE-S3).'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use Server-Side Encryption with Customer Provided Keys (SSE-C).&&Use client-side encryption to provide at-rest encryption.',\n",
       "  'id-question': 'e6e3faf7-8ab8-4780-8b3d-f3bfc9fff5db'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?',\n",
       "  'options': ['Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3',\n",
       "   'Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3',\n",
       "   'Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3',\n",
       "   'Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3',\n",
       "  'id-question': 'fd1490dc-037f-4bd7-afdd-81368ca15e49'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'You are configuring a new VPC for one of your client for a cloud migration project. Only a public VPN will be in place. After you created your VPC, you created a new subnet, a new internet gateway and attached your internet gateway with your VPC. As you created your first instance in to your VPC, you realized that you can not connect the instance even it is configured with elastic IP. What should be done to access the instance?',\n",
       "  'options': ['A NACL should be created and allow all outbound traffic',\n",
       "   'A NAT instance should be created and all traffic should be forwarded to NAT instance',\n",
       "   'Attach another ENI to instance and connect via new ENI',\n",
       "   'A route should be created as 0.0.0.0/0 and your internet gateway as target'],\n",
       "  'img_path': None,\n",
       "  'answer': 'A route should be created as 0.0.0.0/0 and your internet gateway as target',\n",
       "  'id-question': '18885601-b44c-4b68-8c57-3679c29bcfd5'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day. What should a solutions architect do to transmit and process the clickstream data?',\n",
       "  'options': ['Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.',\n",
       "   'Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.',\n",
       "   'Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to process the data for analysis.',\n",
       "   'Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis',\n",
       "  'id-question': '12bb861b-e767-44c5-947e-73a5ff1f510d'},\n",
       " {'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB instances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of configuring and operating this check. What should a solutions architect do to accomplish this?',\n",
       "  'options': ['Use AWS Config rules to define and detect resources that are not properly tagged.',\n",
       "   'Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.',\n",
       "   'Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.',\n",
       "   'Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Use AWS Config rules to define and detect resources that are not properly tagged.',\n",
       "  'id-question': '6fa0c9fe-0ff5-4699-bd37-3aff26ad2aaf'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x : x['id-question'] in selected_id_question,  question_examen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'theme': 'TYPE-EXAMEN',\n",
       "  'question': 'A company is running a business-critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be highly available with minimum downtime and minimum loss of data. Which solution will meet these requirements with the LEAST operational effort?',\n",
       "  'options': ['Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect traffic. Use Aurora PostgreSQL Cross-Region Replication.',\n",
       "   'Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database.',\n",
       "   'Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the database. Recover the database from the snapshots in the event of a failure.',\n",
       "   'Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to Amazon S3. Use S3 Event Notifications to launch an AWS Lambda function to write the data to the database.'],\n",
       "  'img_path': None,\n",
       "  'answer': 'Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database.',\n",
       "  'id-question': '7b6fb120-9dd8-4318-ad47-db3f88140d09'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda q: \"PROXY\" in q[\"answer\"].upper(),question_examen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
